{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNNとは\n",
    "\n",
    "Deep Learningとは、十分なデータ量があれば、人間の力なしに機械が自動的にデータから特徴を抽出してくれるディープニューラルネットワーク（DNN）を用いた学習のことです。  \n",
    "DNNは、ニューラルネットワーク（NN）というパターン認識をするように設計された、人間や動物の脳神経回路をモデルとしたアルゴリズムを多層構造化(層を深くする)したもので、昨今注目を浴びています。  \n",
    "NNを繰り返す繋がりが何度も繰り返されることによって、それは層のように積み重ねられていく。  それが「深層学習」と呼ばれる由来です。(DNNはNNの進化版といった感じです。)  \n",
    "deep learningはDNNを使って学習する技術であり、つまりdeep learningを使う、ということは「DNNを使って学習する技術を使う」ということです  \n",
    "\n",
    "しかし層を深くすることによってある問題が発生が発生してしまいます。  \n",
    "それは**勾配消失問題**です。  \n",
    "DNNによって層を深くすると、何層も重なってるネットワークでバックプロパゲーション学習の重みを掛けていくと、多重に活性化関数(シグモイド関数)が掛かることになって勾配(誤差)が消失してしまうという問題が発生してしまいます。  \n",
    "層を遡るに従って誤差が急速に小さくなり 0 になる(あるいは急速に大きくなって爆発する)ために、学習が制御不能に陥ってしまいます。    \n",
    "また誤差が大きくなり過ぎて爆発してしまうことを**勾配爆発問題**と言います。  \n",
    "その一つの現れが過学習、すなわち学習サンプルに対する誤差(訓練誤差)はいくらでも小さくできるのに、汎化誤差(サンプルの母集団に対する誤差)を小さくできないことです。  \n",
    "この勾配消失問題や過学習を克服する手立てが見つかっ たことが，今のディープネットのブームの根底にあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前回作ったNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = train[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.drop(\"label\",axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display plots inline and change default figure size\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self,input_units,nn_units):\n",
    "        np.random.seed(3)\n",
    "        input_units = X.shape[1]\n",
    "        self.w1 = np.random.randn(input_units,nn_units) / np.sqrt(2)\n",
    "        self.w2 = np.random.randn(nn_units,2) /  np.sqrt(3)\n",
    "        self.b1 = np.zeros((1,nn_units))\n",
    "        self.b2 = np.zeros((1,2))\n",
    "        self.param = { 'w1': self.w1, 'b1': self.b1, 'w2': self.w2, 'b2': self.b2}\n",
    "        \n",
    "    def step(self,x):\n",
    "        if x > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def relu(self,x):\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "    def tanh(self,x):\n",
    "        e = np.exp(x)\n",
    "        e_minus = np.exp(-x)\n",
    "        result = (e-e_minus)/(e+e_minus)\n",
    "        return result\n",
    "    \n",
    "    def softmax(self,a):\n",
    "        c = np.max(a,axis = 0)\n",
    "        e_a = np.exp(a)\n",
    "        e_sum = np.sum(e_a,axis=1, keepdims=True)\n",
    "        y = e_a/e_sum\n",
    "        return y\n",
    "    \n",
    "    def forward_propagation(self):\n",
    "        z1 = np.dot(self.x,self.w1) + self.b1\n",
    "        a1 = self.tanh(z1)\n",
    "        z2 = np.dot(a1,self.w2) + self.b2\n",
    "        y = self.softmax(z2)\n",
    "        return y\n",
    "\n",
    "    def back_propagation(self,learning_rate=0.01):\n",
    "        a1 = self.tanh(np.dot(self.x,self.w1) + self.b1)\n",
    "        delta3 = (self.y_pred-np.identity(2)[self.y])#/len(y)\n",
    "        delta2 = (1-a1**2) * np.dot(delta3,self.w2.T)\n",
    "    \n",
    "        self.w2 -= np.dot(a1.T,delta3)*learning_rate\n",
    "        self.b2 -= np.sum(delta3,axis=0)*learning_rate\n",
    "        self.w1 -= np.dot(self.x.T,delta2)*learning_rate\n",
    "        self.b1 -= np.sum(delta2,axis=0)*learning_rate\n",
    "        return self.w1,self.w2,self.b1,self.b2\n",
    "    \n",
    "    def fit(self,x,y,ite):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.y_pred = self.forward_propagation()\n",
    "        \n",
    "        for i in range(ite):\n",
    "            \n",
    "            self.w1,self.w2,self.b1,self.b2 = self.back_propagation()\n",
    "            self.y_pred = self.forward_propagation()\n",
    "            self.param = { 'w1': self.w1, 'b1': self.b1, 'w2': self.w2, 'b2': self.b2}\n",
    "        return self.param\n",
    "                      \n",
    "    \n",
    "    def predict(self,x):\n",
    "        self.x = x\n",
    "        pred = self.forward_propagation()\n",
    "        return np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizerの種類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n",
    "\n",
    "SGDとは確率的勾配降下法のことです。  \n",
    "目的関数が期待値で表された最適化問題に対して有効な最適化アルゴリズムです。\n",
    "確率的勾配降下法は学習データをシャッフルした上で学習データの中からランダムに1つを取り出して誤差を計算し、パラメーターを更新をします。  \n",
    "勾配降下法ほどの精度は無いが増えた分だけの学習データのみで再学習する(重みベクトルの初期値は前回の学習結果を流用)ため再学習の計算量が圧倒的に低くなります。  \n",
    "運が良ければ最急降下法よりも早く最適解にたどり着けますが、運が悪ければいつまで経っても適した答えを導けません。  \n",
    "### SGDの欠点  \n",
    "・問題によっては非効率  \n",
    "・関数の形状が等方的でないと，勾配が最小値を指さない。  \n",
    "\n",
    "式はこうなります。  \n",
    "$${\\mathbf{w}^{t + 1} \\gets \\mathbf{w}^{t} - \\eta \\frac{\\partial E(\\mathbf{w}^{t})}{\\partial \\mathbf{w}^{t}}\n",
    "}$$\n",
    "\n",
    "\n",
    "## AdaGrad\n",
    "AdaGrad の基本は SGD ですが、学習パラメータwの各成分ごとに異なるlearning rate を与え、あまり更新されていない成分には高い learning rate を割り振るように工夫します。  \n",
    "\n",
    "### AdaGradの長所\n",
    "・ハイパーパラメータが１つだけ  \n",
    "・シンプルで挙動が分かりやすい  \n",
    "・学習率は必ず単調減少するので、勾配が極端に変動するものとかでもあまり変な挙動にならない\n",
    "\n",
    "式はこのようになります\n",
    "$${\\begin{aligned}\n",
    "    \\tilde{v} &\\leftarrow \\tilde{v} + g_{\\theta}^{2} \\\\\n",
    "    \\theta &\\leftarrow \\theta - \\frac{\\alpha}{\\sqrt{\\tilde{v}} + \\epsilon} g_{\\theta}\n",
    "\\end{aligned}\n",
    "}$$\n",
    "\n",
    "\n",
    "## Adam\n",
    "\n",
    "Adamは近年最も有力とされている最適化アルゴリズムです。  \n",
    "Adamでは、これまであまり更新されて来なかったパラメータが優先的に更新されます。  \n",
    "直感的にMomentumとAdaGradを融合したような手法です。  \n",
    "2つの手法の移転を組み合わせることで、効率的にパらメーター空間を探索することが期待できます。  \n",
    "純粋数学的な最適化という観点からすると、これまで最急勾配を選び優先的に下げていく方向だったところに、いや少し待て、それ以外の勾配方向も積極的に試してみようということで、鞍点(多変数実関数の変域の中で、ある方向で見れば極大値だが別の方向で見れば極小値となる点である。)からの抜け出しが速くなります。　　  \n",
    "数式は難しいですが、このようになります。\n",
    "\n",
    "$${m_{t+1} = \\beta_{1} m_{t} + (1 - \\beta_{1}) \\nabla E(\\mathbf{w}^{t})\\\\\n",
    "v_{t+1} = \\beta_{2} v_{t} + (1 - \\beta_{2}) \\nabla E(\\mathbf{w}^{t})^{2}\\\\\n",
    "\\hat{m} = \\frac{m_{t+1}}{1 - \\beta_{1}^{t}}\\\\\n",
    "\\hat{v} = \\frac{v_{t+1}}{1 - \\beta_{2}^{t}}\\\\\n",
    "\\mathbf{w}^{t+1} = \\mathbf{w}^{t} - \\alpha \\frac{\\hat{m}}{\\sqrt{\\hat{v}} + \\epsilon}\n",
    "}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerial_gradient(x,y):\n",
    "    loss_w = lambda w:forward_propagation(x,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SDG(param,grad,learning_rate=0.01):\n",
    "    for key in param.keys():\n",
    "        param[key] = learning_rate * grad[key]\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AdaGrad(param,grad,learning_rate=0.01):\n",
    "    h = {i:0 for i in param.keys()}\n",
    "    for key in param.keys():\n",
    "        h[key] += grad[key] * grad[key]\n",
    "        param[key] = param[key] - learning_rate *grad[key] /(\n",
    "            np.sqrt(h[key])+ 1e-7)\n",
    "    return param,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Adam:\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = np.random.randn(2,3) / np.sqrt(2)\n",
    "w2 = np.random.randn(3,2) /  np.sqrt(3)\n",
    "b1 = np.zeros((1,3))\n",
    "b2 = np.zeros((1,2))\n",
    "param = { 'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2}\n",
    "grad = { 'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重みの初期化\n",
    "今までは最初の重みをランダムで生成していましたが、初期値重みの設定の仕方は様々です。  \n",
    "\n",
    "## Xavier\n",
    "Xavierは層のノードの数によって、作用させる係数を\n",
    "変化させます。  \n",
    "たとえば、前層から渡されるノード数がn個であるときには、標準偏差√nで割rります。  \n",
    "つまり、初期値のバラツキについては、各層ごとにノードの数で均一化しているイメージになるかと思います。  \n",
    "このXavierの初期値は「Sigmoid」か「Tanh」に適している初期値として知られています。   \n",
    "つまり、「ReLU」には最適とはいえません。 \n",
    "\n",
    "\n",
    "# He\n",
    "Xavierの初期値と共によく使われる初期値として「Heの初期値」があります。  \n",
    "作用させる値はXavierと似ていますが、標準偏差√(n/2)で割ります。  \n",
    "「ReLU」を使う時は、それに適した初期値として、Heの初期値を使います。 \n",
    "\n",
    "# ガウス分布\n",
    "左右対称・釣り鐘型の性質をもつ分布として代表的なものが、正規分布（ガウス分布）です。  \n",
    "その名前（正規分布 normal distribution）からもわかる通り、\"normal\"な、「ありふれた」「通常の」確率分布です。  \n",
    "正規分布の最も基本的な性質としては、以下に挙げるものがあります。  \n",
    "・平均値と最頻値と中央値が一致する。  \n",
    "・平均値を中心にして左右対称である。  \n",
    "・分散（標準偏差）が大きくなると、曲線の山は低くなり、左右に広がって平らになる。分散（標準偏差）が小さくなると、山は高くなり、よりとんがった形になる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gauss(input_unit,nn_unit,output_unit):\n",
    "    w1 = np.random.randn(input_unit,nn_unit) * 0.01\n",
    "    w2 = np.random.randn(nn_unit,output_unit) * 0.01\n",
    "    b1 = np.zeros((1,nn_unit))\n",
    "    b2 = np.zeros((1,output_unit))\n",
    "    param = { 'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2}\n",
    "    return param\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Xavier(input_unit,nn_unit,output_unit):\n",
    "    w1 = np.random.randn(input_unit,nn_unit) / np.sqrt(input_unit)\n",
    "    w2 = np.random.randn(nn_unit,output_unit) /  np.sqrt(nn_unit)\n",
    "    b1 = np.zeros((1,nn_unit))\n",
    "    b2 = np.zeros((1,output_unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def He(input_unit,nn_unit,output_unit):\n",
    "    w1 = np.random.randn(input_unit,nn_unit) / (np.sqrt(input_unit)*np.sqrt(2))\n",
    "    w2 = np.random.randn(nn_unit,output_unit) / (np.sqrt(nn_unit)*np.sqrt(2))\n",
    "    b1 = np.zeros((1,nn_unit))\n",
    "    b2 = np.zeros((1,output_unit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch　Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalizationは、Deep Learningにおける各重みパラメータを上手くreparametrizationすることで、ネットワークを最適化するための方法の一つです。近年のイノベーションの中でもかなりアツい手法だと紹介されています。  \n",
    "2015年にIoffe and Szegedyによって発表されました。  \n",
    "基本的には、各ユニットの出力をミニバッチごとに正則化した新たな値で置き直すことで、内部の変数の分布(内部共変量シフト)が大きく変わるのを防ぎ、学習が早くなる、過学習が抑えられるなどの効果が得られます。  \n",
    "その効果はかなり大きく、前述の通りDropoutがいらなくなると言われるレベルとのことです。  \n",
    "簡単に説明すると、各層のアクティベーション分布を強制的に調整する手法です。  \n",
    "数式はこちらになります。\n",
    "$${\\begin{align}\n",
    "\\mu&=\\frac{1}{m}\\sum_i z^{(i)} \\\\\n",
    "\\sigma^2 &= \\frac{1}{m}\\sum_i (z^{(i)}-\\mu)^2 \\\\\n",
    "z_{\\rm norm}^{(i)} &= \\frac{z^{(i)}-\\mu}{\\sqrt{\\sigma^2+\\epsilon}} \\\\\n",
    "\\tilde{z}^{(i)} &= \\gamma z_{\\rm norm}^{(i)}+\\beta\n",
    "\\end{align}\n",
    "}$$\n",
    "\n",
    "### 内部共変量シフト\n",
    "データの分布が訓練時と推定時で異なるような状態のことを言います。  \n",
    "訓練中にネットワーク内の各層の間で起きる共変量シフトを内部共変量シフトと言うようです。  \n",
    "\n",
    "### Reparameterization Trick\n",
    "Reparameterization Trickは変数変換の手法です。  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class bach_norm():    \n",
    "    def forward(self,X, gamma=0.1, beta=1):\n",
    "        mu = np.mean(X, axis=0)\n",
    "        var = np.var(X, axis=0)\n",
    "\n",
    "        X_norm = (X - mu) / np.sqrt(var + 1e-8)\n",
    "        out = gamma * X_norm + beta\n",
    "\n",
    "        cache = (X, X_norm, mu, var, gamma, beta)\n",
    "        self.cache = cache\n",
    "        self.out = out\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self,dout):\n",
    "        X, X_norm, mu, var, gamma, beta = self.cache\n",
    "\n",
    "        N, D = X.shape\n",
    "\n",
    "        X_mu = X - mu\n",
    "        std_inv = 1. / np.sqrt(var + 1e-8)\n",
    "\n",
    "        dX_norm = dout * gamma\n",
    "        dvar = np.sum(dX_norm * X_mu, axis=0) * -.5 * std_inv**3\n",
    "        dmu = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2. * X_mu, axis=0)\n",
    "\n",
    "        dX = (dX_norm * std_inv) + (dvar * 2 * X_mu / N) + (dmu / N)\n",
    "        dgamma = np.sum(dout * X_norm, axis=0)\n",
    "        dbeta = np.sum(dout, axis=0)\n",
    "\n",
    "        dX = np.array(dX)\n",
    "        return dX\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Out\n",
    "Dropoutとは？  \n",
    "Dropoutとは、ニューラルネットワークの学習時に、一定割合のノードを不活性化させながら学習を行うことで過学習を防ぎ（緩和し）、精度をあげるために手法。   \n",
    "ニューラルネットワークは訓練データに対するトレース能力に優れており、わりと簡単に過学習を起こしてしまうため、正則化やDropoutのような手法を用いることは重要である。   \n",
    "学習時に特定のノードを不活性化させて、学習を進めていくことで、過学習を抑えながらパラメーターの更新を行える。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Outのスクラッチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前回のNNで作った活性化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dX = dout\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss():\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        \n",
    "    def softmax(self,a):\n",
    "        c = np.max(a,axis = 0)\n",
    "        e_a = np.exp(a-c)\n",
    "        e_sum = np.sum(e_a,axis=1, keepdims=True)\n",
    "        y = e_a/e_sum\n",
    "        return y\n",
    "    \n",
    "    def cross_entropy_error(self,y,t):\n",
    "        delta = 1e-7\n",
    "        result = -np.sum(t*np.log(y + delta))/len(y)\n",
    "        return result\n",
    "        \n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(x)\n",
    "        self.loss = self.cross_entropy_error(self.y,self.t)\n",
    "        return self.y\n",
    "        \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t)/ batch_size      \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 活性化関数の前に挟むAffine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self,w,b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        print(np.argmax(x,axis = 1))\n",
    "        print(\"__________________________\")        \n",
    "        self.x = x\n",
    "        out = np.dot(self.x,self.w) + self.b\n",
    "        print(np.argmax(out,axis = 1),out.shape)\n",
    "        print(\"__________________________\")\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = np.dot(dout,self.w.T)\n",
    "        self.dw = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout, axis = 0)\n",
    "        return dx\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "レイヤーを辞書に入れて値を保存し、バックプロパゲーション時に引き出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "class DNN():\n",
    "    def __init__(self,layer_node):\n",
    "        self.masks = []\n",
    "        np.random.seed(1)\n",
    "        self.node = layer_node\n",
    "        self.n = len(layer_node)\n",
    "        self.wight_params = OrderedDict()\n",
    "        self.params = {\"w\" + str(i+1) :np.random.randn(t[0],t[1]) / np.sqrt(t[0]) *np.sqrt(2)\n",
    "                  for i,t in zip(range(len(layer_node)),layer_node)}\n",
    "        self.params.update({\"b\"+ str(i+1):np.zeros((1,t[1]))\n",
    "                            for i,t in zip(range(len(layer_node)),layer_node)})\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers[\"Affine1\"] = Affine(self.params[\"w1\"],self.params[\"b1\"])\n",
    "        for i in range(self.n-1):\n",
    "            i += 1\n",
    "            print(i)\n",
    "            self.layers[\"bach_norm\"+str(i)] = bach_norm(self.params[\"w\"+str(i)],self.params[\"b\"+str(i)])\n",
    "            self.layers[\"Relu\"+str(i)] = Relu()\n",
    "            self.layers[\"Affine\"+str(i+1)] =  Affine(self.params[\"w\"+str(i+1)],self.params[\"b\"+str(i+1)])\n",
    "        \n",
    "        self.lastLayers = SoftmaxWithLoss()\n",
    "    \n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layers.forward(x)\n",
    "        \n",
    "        return(x)\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayers.forward(y,t)\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis = 1)\n",
    "            \n",
    "        accuracy = np.sum(y == t) / float(t, axis = 1)\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)    \n",
    "        \n",
    "        grads = {}\n",
    "        for i in range(self.n):\n",
    "            i += 1\n",
    "            grads['w'+str(i)] = numerical_gradient(loss_W, self.params['w'+str(i)])\n",
    "            grads['b'+str(i)] = numerical_gradient(loss_W, self.params['b'+str(i)])\n",
    "    \n",
    "    \n",
    "    def gradient(self,x,t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        grads = {}\n",
    "        for i in range(self.n):\n",
    "            i += 1\n",
    "            grads[\"w\"+str(i)] = self.layers[\"Affine\"+str(i)].dw\n",
    "            grads[\"b\"+str(i)] = self.layers[\"Affine\"+str(i)].db\n",
    "        \n",
    "        return grads\n",
    "            \n",
    "\n",
    "\n",
    "    def forward_propagation(self,x):\n",
    "        self.x = x\n",
    "        self.out = self.x.copy\n",
    "        for i in range(self.n-1):\n",
    "            self.w = self.params[\"w_\" + str(i+1)]\n",
    "            self.b = self.params[\"b_\" + str(i+1)]\n",
    "            self.out = Affine.forward(self,self.x)\n",
    "            self.out, cache, mu, var = batchnorm_forward(self.out, 0.1, 1)\n",
    "            self.x = Relu.forward(self,self.out)\n",
    "        \n",
    "        self.w = self.params[\"w_\" + str(self.n)]\n",
    "        self.b = self.params[\"b_\" + str(self.n)]\n",
    "        self.out = Affine.forward(self,self.x)\n",
    "        self.y = softmaxwithLoss.softmax(self,self.out)\n",
    "        return self.y\n",
    "    \n",
    "    def back_propagation(self,t,learning_rate=0.1):\n",
    "        self.t = np.identity(self.node[-1][-1])[t]\n",
    "        self.w = self.params[\"w_\" + str(self.n)]\n",
    "        self.b = self.params[\"b_\" + str(self.n)]\n",
    "        self.out = softmaxwithLoss.backwarf(self)\n",
    "        self.out = Affine.backward(self,self.out)\n",
    "        self.param_change(0)\n",
    "        \n",
    "        for i in range(self.n-1):\n",
    "            self.w = self.params[\"w_\" + str(self.n-i-1)]\n",
    "            self.b = self.params[\"b_\" + str(self.n-i-1)]\n",
    "            self.out = Relu.backward(self,self.out,self.n-i-1)\n",
    "            self.out = Affine.backward(self,self.out)    \n",
    "            self.param_change(i)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "    def param_change(self,i,learning_rate = 0.1):\n",
    "        self.w  -= self.dw * learning_rate\n",
    "        self.b  -= self.db * learning_rate\n",
    "        self.params[\"w_\" + str(self.n-i)] = self.w\n",
    "        self.params[\"b_\" + str(self.n-i)] = self.b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ここまででスクラッチは終わりました。　　　　  \n",
    "実際に別のファイルでこのコードと教科書のコードを少しだけ借りて動かしてみたいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
