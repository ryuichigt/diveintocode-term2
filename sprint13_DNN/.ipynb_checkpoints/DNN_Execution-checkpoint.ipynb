{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 必要なライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 活性化関数の関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x > 0, dtype=np.int)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))    \n",
    "\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "    \n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_grad(x):\n",
    "    grad = np.zeros(x)\n",
    "    grad[x>=0] = 1\n",
    "    return grad\n",
    "    \n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "def softmax_loss(X, t):\n",
    "    y = softmax(X)\n",
    "    return cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def smooth_curve(x):\n",
    "    \"\"\"損失関数のグラフを滑らかにするために用いる\n",
    "    \"\"\"\n",
    "    window_len = 11\n",
    "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
    "    w = np.kaiser(window_len, 2)\n",
    "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
    "    return y[5:len(y)-5]\n",
    "\n",
    "\n",
    "def shuffle_dataset(x, t):\n",
    "    \"\"\"データセットのシャッフルを行う\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 訓練データ\n",
    "    t : 教師データ\n",
    "    Returns\n",
    "    -------\n",
    "    x, t : シャッフルを行った訓練データと教師データ\n",
    "    \"\"\"\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
    "    t = t[permutation]\n",
    "\n",
    "    return x, t\n",
    "\n",
    "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
    "    return (input_size + 2*pad - filter_size) / stride + 1\n",
    "\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 入力データの形状（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SGD:\n",
    "\n",
    "    \"\"\"確率的勾配降下法（Stochastic Gradient Descent）\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] \n",
    "\n",
    "\n",
    "class Momentum:\n",
    "\n",
    "    \"\"\"Momentum SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]\n",
    "\n",
    "\n",
    "class Nesterov:\n",
    "\n",
    "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.v[key] *= self.momentum\n",
    "            self.v[key] -= self.lr * grads[key]\n",
    "            params[key] += self.momentum * self.momentum * self.v[key]\n",
    "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "\n",
    "    \"\"\"RMSprop\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 活性化関数　クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 重み・バイアスパラメータの微分\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # テンソル対応\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 入力データの形状に戻す（テンソル対応）\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ドロップアウト バッチノーマライズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Dropout:\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "class BatchNormalization:\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1502.03167\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.input_shape = None # Conv層の場合は4次元、全結合層の場合は2次元  \n",
    "\n",
    "        # テスト時に使用する平均と分散\n",
    "        self.running_mean = running_mean\n",
    "        self.running_var = running_var  \n",
    "        \n",
    "        # backward時に使用する中間データ\n",
    "        self.batch_size = None\n",
    "        self.xc = None\n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim != 2:\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.reshape(N, -1)\n",
    "\n",
    "        out = self.__forward(x, train_flg)\n",
    "        \n",
    "        return out.reshape(*self.input_shape)\n",
    "            \n",
    "    def __forward(self, x, train_flg):\n",
    "        if self.running_mean is None:\n",
    "            N, D = x.shape\n",
    "            self.running_mean = np.zeros(D)\n",
    "            self.running_var = np.zeros(D)\n",
    "                        \n",
    "        if train_flg:\n",
    "            mu = x.mean(axis=0)\n",
    "            xc = x - mu\n",
    "            var = np.mean(xc**2, axis=0)\n",
    "            std = np.sqrt(var + 10e-7)\n",
    "            xn = xc / std\n",
    "            \n",
    "            self.batch_size = x.shape[0]\n",
    "            self.xc = xc\n",
    "            self.xn = xn\n",
    "            self.std = std\n",
    "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
    "            \n",
    "        out = self.gamma * xn + self.beta \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if dout.ndim != 2:\n",
    "            N, C, H, W = dout.shape\n",
    "            dout = dout.reshape(N, -1)\n",
    "\n",
    "        dx = self.__backward(dout)\n",
    "\n",
    "        dx = dx.reshape(*self.input_shape)\n",
    "        return dx\n",
    "\n",
    "    def __backward(self, dout):\n",
    "        dbeta = dout.sum(axis=0)\n",
    "        dgamma = np.sum(self.xn * dout, axis=0)\n",
    "        dxn = self.gamma * dout\n",
    "        dxc = dxn / self.std\n",
    "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
    "        dvar = 0.5 * dstd / self.std\n",
    "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
    "        dmu = np.sum(dxc, axis=0)\n",
    "        dx = dxc - dmu / self.batch_size\n",
    "        \n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "        \n",
    "        return dx\n",
    "\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 中間データ（backward時に使用）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 重み・バイアスパラメータの勾配\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class MultiLayerNetExtend:\n",
    "    \"\"\"拡張版の全結合による多層ニューラルネットワーク\n",
    "    \n",
    "    Weiht Decay、Dropout、Batch Normalizationの機能を持つ\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    weight_decay_lambda : Weight Decay（L2ノルム）の強さ\n",
    "    use_dropout: Dropoutを使用するかどうか\n",
    "    dropout_ration : Dropoutの割り合い\n",
    "    use_batchNorm: Batch Normalizationを使用するかどうか\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size_list, output_size,\n",
    "                 activation='relu', weight_init_std='relu', weight_decay_lambda=0, \n",
    "                 use_dropout = False, dropout_ration = 0.5, use_batchnorm=False):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "        self.use_dropout = use_dropout\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.params = {}\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.__init_weight(weight_init_std)\n",
    "\n",
    "        # レイヤの生成\n",
    "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
    "        self.layers = OrderedDict()\n",
    "        for idx in range(1, self.hidden_layer_num+1):\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "                                                      self.params['b' + str(idx)])\n",
    "            if self.use_batchnorm:\n",
    "                self.params['gamma' + str(idx)] = np.ones(hidden_size_list[idx-1])\n",
    "                self.params['beta' + str(idx)] = np.zeros(hidden_size_list[idx-1])\n",
    "                self.layers['BatchNorm' + str(idx)] = BatchNormalization(self.params['gamma' + str(idx)], self.params['beta' + str(idx)])\n",
    "                \n",
    "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
    "            \n",
    "            if self.use_dropout:\n",
    "                self.layers['Dropout' + str(idx)] = Dropout(dropout_ration)\n",
    "\n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def __init_weight(self, weight_init_std):\n",
    "        \"\"\"重みの初期値設定\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "            'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "            'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "        \"\"\"\n",
    "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        for idx in range(1, len(all_size_list)):\n",
    "            scale = weight_init_std\n",
    "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
    "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLUを使う場合に推奨される初期値\n",
    "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
    "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoidを使う場合に推奨される初期値\n",
    "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key or \"BatchNorm\" in key:\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t, train_flg=False):\n",
    "        \"\"\"損失関数を求める\n",
    "        引数のxは入力データ、tは教師ラベル\n",
    "        \"\"\"\n",
    "        y = self.predict(x, train_flg)\n",
    "\n",
    "        weight_decay = 0\n",
    "        for idx in range(1, self.hidden_layer_num + 2):\n",
    "            W = self.params['W' + str(idx)]\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "\n",
    "        return self.last_layer.forward(y, t) + weight_decay\n",
    "\n",
    "    def accuracy(self, X, T):\n",
    "        Y = self.predict(X, train_flg=False)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        if T.ndim != 1 : T = np.argmax(T, axis=1)\n",
    "\n",
    "        accuracy = np.sum(Y == T) / float(X.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, X, T):\n",
    "        \"\"\"勾配を求める（数値微分）\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 入力データ\n",
    "        T : 教師ラベル\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        loss_W = lambda W: self.loss(X, T, train_flg=True)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n",
    "            \n",
    "            if self.use_batchnorm and idx != self.hidden_layer_num+1:\n",
    "                grads['gamma' + str(idx)] = numerical_gradient(loss_W, self.params['gamma' + str(idx)])\n",
    "                grads['beta' + str(idx)] = numerical_gradient(loss_W, self.params['beta' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t, train_flg=True)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.params['W' + str(idx)]\n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "\n",
    "            if self.use_batchnorm and idx != self.hidden_layer_num+1:\n",
    "                grads['gamma' + str(idx)] = self.layers['BatchNorm' + str(idx)].dgamma\n",
    "                grads['beta' + str(idx)] = self.layers['BatchNorm' + str(idx)].dbeta\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    \"\"\"ニューラルネットの訓練を行うクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
    "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
    "        self.network = network\n",
    "        self.verbose = verbose\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
    "\n",
    "        # optimizer\n",
    "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
    "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "        \n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
    "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "        \n",
    "        grads = self.network.gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.network.params, grads)\n",
    "        \n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "        if self.verbose: print(\"train loss:\" + str(loss))\n",
    "        \n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
    "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
    "            if not self.evaluate_sample_num_per_epoch is None:\n",
    "                t = self.evaluate_sample_num_per_epoch\n",
    "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
    "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
    "                \n",
    "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
    "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "\n",
    "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = train[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(\"label\",axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishikawaryuuichi/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, t_train, t_test = train_test_split(train,target, train_size=0.7, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:14.3451051184\n",
      "=== epoch:1, train acc:0.12, test acc:0.0977700182525 ===\n",
      "train loss:13.3415230441\n",
      "train loss:12.4860534827\n",
      "train loss:13.05120961\n",
      "=== epoch:2, train acc:0.123333333333, test acc:0.098563606063 ===\n",
      "train loss:12.2519546682\n",
      "train loss:8.84894155247\n",
      "train loss:7.94838849327\n",
      "=== epoch:3, train acc:0.136666666667, test acc:0.100230140465 ===\n",
      "train loss:6.29179369953\n",
      "train loss:5.38846264754\n",
      "train loss:4.01320474906\n",
      "=== epoch:4, train acc:0.14, test acc:0.10364256805 ===\n",
      "train loss:3.65896981944\n",
      "train loss:3.0297041775\n",
      "train loss:3.03163922592\n",
      "=== epoch:5, train acc:0.153333333333, test acc:0.10967383541 ===\n",
      "train loss:2.8828895954\n",
      "train loss:2.51782742263\n",
      "train loss:2.5011142942\n",
      "=== epoch:6, train acc:0.156666666667, test acc:0.116101896675 ===\n",
      "train loss:2.30544861902\n",
      "train loss:2.35687544964\n",
      "train loss:2.40813161077\n",
      "=== epoch:7, train acc:0.156666666667, test acc:0.126656614554 ===\n",
      "train loss:2.31677467128\n",
      "train loss:2.29088640107\n",
      "train loss:2.27480369645\n",
      "=== epoch:8, train acc:0.166666666667, test acc:0.135624156813 ===\n",
      "train loss:2.2774285827\n",
      "train loss:2.22639105766\n",
      "train loss:2.27293726363\n",
      "=== epoch:9, train acc:0.166666666667, test acc:0.143004523451 ===\n",
      "train loss:2.23793314414\n",
      "train loss:2.2263569136\n",
      "train loss:2.20424849501\n",
      "=== epoch:10, train acc:0.18, test acc:0.144909134196 ===\n",
      "train loss:2.20688168335\n",
      "train loss:2.19753356374\n",
      "train loss:2.1990507108\n",
      "=== epoch:11, train acc:0.173333333333, test acc:0.154035394016 ===\n",
      "train loss:2.20755960579\n",
      "train loss:2.20072876353\n",
      "train loss:2.14219902057\n",
      "=== epoch:12, train acc:0.18, test acc:0.164986905801 ===\n",
      "train loss:2.2085405551\n",
      "train loss:2.23490933659\n",
      "train loss:2.25739512448\n",
      "=== epoch:13, train acc:0.19, test acc:0.169272279978 ===\n",
      "train loss:2.1469133321\n",
      "train loss:2.22078862562\n",
      "train loss:2.19986091662\n",
      "=== epoch:14, train acc:0.2, test acc:0.17927148639 ===\n",
      "train loss:2.19568150529\n",
      "train loss:2.17382272786\n",
      "train loss:2.11831494143\n",
      "=== epoch:15, train acc:0.22, test acc:0.186255059122 ===\n",
      "train loss:2.11362618297\n",
      "train loss:2.11737830154\n",
      "train loss:2.05491159929\n",
      "=== epoch:16, train acc:0.25, test acc:0.192365685263 ===\n",
      "train loss:2.12275222423\n",
      "train loss:2.16977740356\n",
      "train loss:2.1094619228\n",
      "=== epoch:17, train acc:0.273333333333, test acc:0.197762082374 ===\n",
      "train loss:2.21718064222\n",
      "train loss:2.13077971246\n",
      "train loss:2.17475707233\n",
      "=== epoch:18, train acc:0.286666666667, test acc:0.201253868741 ===\n",
      "train loss:2.11674769488\n",
      "train loss:2.14894447716\n",
      "train loss:2.15189276118\n",
      "=== epoch:19, train acc:0.3, test acc:0.209745258313 ===\n",
      "train loss:2.11240452521\n",
      "train loss:2.18274013521\n",
      "train loss:2.08100464904\n",
      "=== epoch:20, train acc:0.33, test acc:0.219427029601 ===\n",
      "train loss:2.1552335516\n",
      "train loss:2.12834084573\n",
      "train loss:2.09831788943\n",
      "=== epoch:21, train acc:0.32, test acc:0.221728434251 ===\n",
      "train loss:2.01823980596\n",
      "train loss:2.13825360709\n",
      "train loss:2.15806925083\n",
      "=== epoch:22, train acc:0.32, test acc:0.234425839219 ===\n",
      "train loss:2.04860188183\n",
      "train loss:2.05454103004\n",
      "train loss:1.99182840187\n",
      "=== epoch:23, train acc:0.336666666667, test acc:0.243472740259 ===\n",
      "train loss:2.01850054929\n",
      "train loss:2.0977411432\n",
      "train loss:2.16720392879\n",
      "=== epoch:24, train acc:0.36, test acc:0.249980160305 ===\n",
      "train loss:2.02134743385\n",
      "train loss:2.07446745481\n",
      "train loss:1.99089165774\n",
      "=== epoch:25, train acc:0.37, test acc:0.260455519403 ===\n",
      "train loss:2.00720257118\n",
      "train loss:1.97723304689\n",
      "train loss:2.03701277548\n",
      "=== epoch:26, train acc:0.36, test acc:0.262439488929 ===\n",
      "train loss:2.05516328238\n",
      "train loss:2.05798466457\n",
      "train loss:2.086122435\n",
      "=== epoch:27, train acc:0.37, test acc:0.274422664868 ===\n",
      "train loss:1.97111791518\n",
      "train loss:1.99882873929\n",
      "train loss:1.9543583503\n",
      "=== epoch:28, train acc:0.38, test acc:0.281247520038 ===\n",
      "train loss:1.96729233238\n",
      "train loss:2.02515916395\n",
      "train loss:1.98042555037\n",
      "=== epoch:29, train acc:0.373333333333, test acc:0.292992619633 ===\n",
      "train loss:1.97758288153\n",
      "train loss:1.97805267603\n",
      "train loss:2.02557002171\n",
      "=== epoch:30, train acc:0.39, test acc:0.299738116023 ===\n",
      "train loss:2.03282708029\n",
      "train loss:2.00490772806\n",
      "train loss:1.97052951146\n",
      "=== epoch:31, train acc:0.39, test acc:0.308150146814 ===\n",
      "train loss:1.98978483919\n",
      "train loss:1.93088040834\n",
      "train loss:1.96917916568\n",
      "=== epoch:32, train acc:0.41, test acc:0.317355765415 ===\n",
      "train loss:1.98810578303\n",
      "train loss:1.97106443594\n",
      "train loss:1.95018467234\n",
      "=== epoch:33, train acc:0.413333333333, test acc:0.322910880089 ===\n",
      "train loss:2.00162177137\n",
      "train loss:1.95801046144\n",
      "train loss:1.94784954247\n",
      "=== epoch:34, train acc:0.446666666667, test acc:0.331957781128 ===\n",
      "train loss:2.00584584582\n",
      "train loss:1.90794939785\n",
      "train loss:1.91612078286\n",
      "=== epoch:35, train acc:0.436666666667, test acc:0.336878025554 ===\n",
      "train loss:1.906905364\n",
      "train loss:1.86387143261\n",
      "train loss:1.85832249383\n",
      "=== epoch:36, train acc:0.453333333333, test acc:0.349575430521 ===\n",
      "train loss:1.97126118075\n",
      "train loss:1.8755958195\n",
      "train loss:1.87397440133\n",
      "=== epoch:37, train acc:0.446666666667, test acc:0.352194270296 ===\n",
      "train loss:1.83829602701\n",
      "train loss:1.90462161444\n",
      "train loss:1.96735404012\n",
      "=== epoch:38, train acc:0.466666666667, test acc:0.361955400365 ===\n",
      "train loss:1.88339625232\n",
      "train loss:1.89399147266\n",
      "train loss:1.90315956627\n",
      "=== epoch:39, train acc:0.473333333333, test acc:0.371637171653 ===\n",
      "train loss:1.88939421551\n",
      "train loss:1.86902974016\n",
      "train loss:1.80539263871\n",
      "=== epoch:40, train acc:0.456666666667, test acc:0.371319736529 ===\n",
      "train loss:1.84908163419\n",
      "train loss:1.88844667533\n",
      "train loss:1.88552217397\n",
      "=== epoch:41, train acc:0.486666666667, test acc:0.386794698833 ===\n",
      "train loss:1.80702421542\n",
      "train loss:1.84079757136\n",
      "train loss:1.86050717615\n",
      "=== epoch:42, train acc:0.503333333333, test acc:0.390841996667 ===\n",
      "train loss:1.79433637297\n",
      "train loss:1.8974596706\n",
      "train loss:1.88890613807\n",
      "=== epoch:43, train acc:0.513333333333, test acc:0.397666851837 ===\n",
      "train loss:1.94184228954\n",
      "train loss:1.798843105\n",
      "train loss:1.75409004087\n",
      "=== epoch:44, train acc:0.516666666667, test acc:0.405920165066 ===\n",
      "train loss:1.85292172392\n",
      "train loss:1.80323084864\n",
      "train loss:1.81214430444\n",
      "=== epoch:45, train acc:0.52, test acc:0.409173875089 ===\n",
      "train loss:1.84208135317\n",
      "train loss:1.85879909917\n",
      "train loss:1.84177301776\n",
      "=== epoch:46, train acc:0.53, test acc:0.417744623443 ===\n",
      "train loss:1.85352527611\n",
      "train loss:1.82156122389\n",
      "train loss:1.80377399739\n",
      "=== epoch:47, train acc:0.536666666667, test acc:0.420760257122 ===\n",
      "train loss:1.83698035006\n",
      "train loss:1.85437386833\n",
      "train loss:1.82201114675\n",
      "=== epoch:48, train acc:0.556666666667, test acc:0.423934608364 ===\n",
      "train loss:1.87855239058\n",
      "train loss:1.82030092852\n",
      "train loss:1.82176274262\n",
      "=== epoch:49, train acc:0.546666666667, test acc:0.427188318387 ===\n",
      "train loss:1.78317578467\n",
      "train loss:1.75516317951\n",
      "train loss:1.77848901945\n",
      "=== epoch:50, train acc:0.56, test acc:0.432029204031 ===\n",
      "train loss:1.82153761036\n",
      "train loss:1.83476426276\n",
      "train loss:1.75023677474\n",
      "=== epoch:51, train acc:0.586666666667, test acc:0.443853662408 ===\n",
      "train loss:1.78851860893\n",
      "train loss:1.78061775354\n",
      "train loss:1.73053025261\n",
      "=== epoch:52, train acc:0.586666666667, test acc:0.447424807555 ===\n",
      "train loss:1.68948607705\n",
      "train loss:1.80064829843\n",
      "train loss:1.67613779811\n",
      "=== epoch:53, train acc:0.596666666667, test acc:0.456233632251 ===\n",
      "train loss:1.71544200963\n",
      "train loss:1.79716294827\n",
      "train loss:1.66219912045\n",
      "=== epoch:54, train acc:0.596666666667, test acc:0.460042853742 ===\n",
      "train loss:1.75115079467\n",
      "train loss:1.7722155952\n",
      "train loss:1.81717632062\n",
      "=== epoch:55, train acc:0.593333333333, test acc:0.460042853742 ===\n",
      "train loss:1.76483720048\n",
      "train loss:1.72751048449\n",
      "train loss:1.78024673419\n",
      "=== epoch:56, train acc:0.633333333333, test acc:0.473375128958 ===\n",
      "train loss:1.73528114979\n",
      "train loss:1.6694054637\n",
      "train loss:1.66723790649\n",
      "=== epoch:57, train acc:0.64, test acc:0.479723831442 ===\n",
      "train loss:1.7582209509\n",
      "train loss:1.69318291137\n",
      "train loss:1.78937750182\n",
      "=== epoch:58, train acc:0.646666666667, test acc:0.485278946115 ===\n",
      "train loss:1.68155710822\n",
      "train loss:1.71085748862\n",
      "train loss:1.71975122363\n",
      "=== epoch:59, train acc:0.653333333333, test acc:0.485517022459 ===\n",
      "train loss:1.69524094783\n",
      "train loss:1.6452861641\n",
      "train loss:1.64611702638\n",
      "=== epoch:60, train acc:0.636666666667, test acc:0.494167129593 ===\n",
      "train loss:1.7475612804\n",
      "train loss:1.68649425034\n",
      "train loss:1.69573878294\n",
      "=== epoch:61, train acc:0.65, test acc:0.505515435283 ===\n",
      "train loss:1.55042286753\n",
      "train loss:1.63061375973\n",
      "train loss:1.66039242602\n",
      "=== epoch:62, train acc:0.65, test acc:0.506309023093 ===\n",
      "train loss:1.62767997928\n",
      "train loss:1.65355377967\n",
      "train loss:1.61993422986\n",
      "=== epoch:63, train acc:0.66, test acc:0.502420442822 ===\n",
      "train loss:1.68516802878\n",
      "train loss:1.7110719123\n",
      "train loss:1.71890763057\n",
      "=== epoch:64, train acc:0.663333333333, test acc:0.512816443139 ===\n",
      "train loss:1.65844646478\n",
      "train loss:1.63244610402\n",
      "train loss:1.6175976778\n",
      "=== epoch:65, train acc:0.67, test acc:0.51964129831 ===\n",
      "train loss:1.54742650769\n",
      "train loss:1.73850763091\n",
      "train loss:1.59378620778\n",
      "=== epoch:66, train acc:0.66, test acc:0.520911038806 ===\n",
      "train loss:1.59135457287\n",
      "train loss:1.60755253647\n",
      "train loss:1.56191873966\n",
      "=== epoch:67, train acc:0.67, test acc:0.523609237362 ===\n",
      "train loss:1.68591702838\n",
      "train loss:1.63989712591\n",
      "train loss:1.63313203487\n",
      "=== epoch:68, train acc:0.683333333333, test acc:0.527418458852 ===\n",
      "train loss:1.70999686396\n",
      "train loss:1.58523004264\n",
      "train loss:1.63492768247\n",
      "=== epoch:69, train acc:0.7, test acc:0.531465756686 ===\n",
      "train loss:1.58273057664\n",
      "train loss:1.5703720441\n",
      "train loss:1.6009445413\n",
      "=== epoch:70, train acc:0.696666666667, test acc:0.531624474248 ===\n",
      "train loss:1.66870264289\n",
      "train loss:1.69783968746\n",
      "train loss:1.57120091875\n",
      "=== epoch:71, train acc:0.7, test acc:0.534560749147 ===\n",
      "train loss:1.68089881237\n",
      "train loss:1.53647785462\n",
      "train loss:1.58482743612\n",
      "=== epoch:72, train acc:0.693333333333, test acc:0.534322672804 ===\n",
      "train loss:1.58475887874\n",
      "train loss:1.58042552173\n",
      "train loss:1.53187954271\n",
      "=== epoch:73, train acc:0.713333333333, test acc:0.540750734069 ===\n",
      "train loss:1.49934506154\n",
      "train loss:1.62947534693\n",
      "train loss:1.62332726671\n",
      "=== epoch:74, train acc:0.713333333333, test acc:0.542655344814 ===\n",
      "train loss:1.5410100239\n",
      "train loss:1.57717673226\n",
      "train loss:1.54216376718\n",
      "=== epoch:75, train acc:0.72, test acc:0.548607253393 ===\n",
      "train loss:1.5186887782\n",
      "train loss:1.64951098799\n",
      "train loss:1.66816719094\n",
      "=== epoch:76, train acc:0.716666666667, test acc:0.551781604635 ===\n",
      "train loss:1.48597067922\n",
      "train loss:1.4132343119\n",
      "train loss:1.56398731168\n",
      "=== epoch:77, train acc:0.716666666667, test acc:0.554162368066 ===\n",
      "train loss:1.54561038749\n",
      "train loss:1.49421060838\n",
      "train loss:1.58182775856\n",
      "=== epoch:78, train acc:0.733333333333, test acc:0.55447980319 ===\n",
      "train loss:1.54083578423\n",
      "train loss:1.58228886623\n",
      "train loss:1.57754839757\n",
      "=== epoch:79, train acc:0.733333333333, test acc:0.562495040076 ===\n",
      "train loss:1.58025127525\n",
      "train loss:1.53193813109\n",
      "train loss:1.42574789693\n",
      "=== epoch:80, train acc:0.733333333333, test acc:0.564558368383 ===\n",
      "train loss:1.57533845419\n",
      "train loss:1.57799423183\n",
      "train loss:1.49029799607\n",
      "=== epoch:81, train acc:0.74, test acc:0.570827712086 ===\n",
      "train loss:1.56170662872\n",
      "train loss:1.45576526827\n",
      "train loss:1.48967929188\n",
      "=== epoch:82, train acc:0.746666666667, test acc:0.572018093802 ===\n",
      "train loss:1.51395818403\n",
      "train loss:1.42964089474\n",
      "train loss:1.37923080063\n",
      "=== epoch:83, train acc:0.756666666667, test acc:0.580509483374 ===\n",
      "train loss:1.47819189175\n",
      "train loss:1.40826379187\n",
      "train loss:1.53046170945\n",
      "=== epoch:84, train acc:0.763333333333, test acc:0.585747162924 ===\n",
      "train loss:1.49018913455\n",
      "train loss:1.48921575566\n",
      "train loss:1.41239010538\n",
      "=== epoch:85, train acc:0.753333333333, test acc:0.584239346084 ===\n",
      "train loss:1.41529466044\n",
      "train loss:1.52141440113\n",
      "train loss:1.4277201256\n",
      "=== epoch:86, train acc:0.773333333333, test acc:0.585271010237 ===\n",
      "train loss:1.45810773806\n",
      "train loss:1.45714930113\n",
      "train loss:1.37865627937\n",
      "=== epoch:87, train acc:0.77, test acc:0.590508689787 ===\n",
      "train loss:1.39601918768\n",
      "train loss:1.40084753581\n",
      "train loss:1.43374585019\n",
      "=== epoch:88, train acc:0.783333333333, test acc:0.597809697643 ===\n",
      "train loss:1.40017499059\n",
      "train loss:1.44340750044\n",
      "train loss:1.52269424251\n",
      "=== epoch:89, train acc:0.78, test acc:0.601222125228 ===\n",
      "train loss:1.44497665057\n",
      "train loss:1.32124520155\n",
      "train loss:1.43089473074\n",
      "=== epoch:90, train acc:0.79, test acc:0.601618919133 ===\n",
      "train loss:1.43958055798\n",
      "train loss:1.37248344599\n",
      "train loss:1.41287215085\n",
      "=== epoch:91, train acc:0.79, test acc:0.597333544957 ===\n",
      "train loss:1.35925796909\n",
      "train loss:1.47235614178\n",
      "train loss:1.40695567889\n",
      "=== epoch:92, train acc:0.793333333333, test acc:0.603206094754 ===\n",
      "train loss:1.42918542198\n",
      "train loss:1.30795820476\n",
      "train loss:1.46962871663\n",
      "=== epoch:93, train acc:0.783333333333, test acc:0.60360288866 ===\n",
      "train loss:1.36233700447\n",
      "train loss:1.33739820411\n",
      "train loss:1.34712628187\n",
      "=== epoch:94, train acc:0.783333333333, test acc:0.606459804777 ===\n",
      "train loss:1.44645141423\n",
      "train loss:1.37767146325\n",
      "train loss:1.48689614545\n",
      "=== epoch:95, train acc:0.786666666667, test acc:0.614316324101 ===\n",
      "train loss:1.39251520806\n",
      "train loss:1.29294305081\n",
      "train loss:1.37139122357\n",
      "=== epoch:96, train acc:0.81, test acc:0.618046186811 ===\n",
      "train loss:1.32913924167\n",
      "train loss:1.44057464256\n",
      "train loss:1.25620275744\n",
      "=== epoch:97, train acc:0.79, test acc:0.615189270693 ===\n",
      "train loss:1.305806247\n",
      "train loss:1.29062664391\n",
      "train loss:1.31661037346\n",
      "=== epoch:98, train acc:0.793333333333, test acc:0.620030156337 ===\n",
      "train loss:1.32259667033\n",
      "train loss:1.33508850696\n",
      "train loss:1.28613753121\n",
      "=== epoch:99, train acc:0.81, test acc:0.622490278549 ===\n",
      "train loss:1.23403698866\n",
      "train loss:1.14429911842\n",
      "train loss:1.33021780186\n",
      "=== epoch:100, train acc:0.816666666667, test acc:0.623998095389 ===\n",
      "train loss:1.26077451374\n",
      "train loss:1.22395714557\n",
      "train loss:1.20729153991\n",
      "=== epoch:101, train acc:0.81, test acc:0.627886675661 ===\n",
      "train loss:1.39880174517\n",
      "train loss:1.25243528087\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.631140385684\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKeWkSUEppZSTJgWllFJO/w+tbl3g/W9EwgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117ddacc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "\n",
    "# 過学習を再現するために、学習データを削減\n",
    "x_train = x_train[:1000]\n",
    "t_train = t_train[:1000]\n",
    "\n",
    "# Dropuoutの有無、割り合いの設定 ========================\n",
    "use_dropout = True  # Dropoutなしのときの場合はFalseに\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio,use_batchnorm = True)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=101, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# グラフの描画==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "過学習は起こしていますが、しっかり学習できてることが確認できました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
